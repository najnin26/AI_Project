{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4972c7",
   "metadata": {},
   "source": [
    "# **Loading tools and dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea8cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 22:29:55.011045: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 22:29:55.047448: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 22:29:55.648882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be3d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data=pd.read_csv('arxiv_data_210930-054931.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0244c3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74d0b25",
   "metadata": {},
   "source": [
    "# **Data Cleaning and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a81cf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30bf0c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b3cfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>['cs.CV', 'cs.IR']</td>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "      <td>Despite the growing availability of big data i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']</td>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "      <td>This paper presents a simple end-to-end model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "      <td>The popular Q-learning algorithm is known to o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>['stat.ML', 'cs.LG', 'math.OC']</td>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "      <td>Principal components analysis (PCA) is a well-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>['cs.LG', 'cs.AI', 'stat.ML']</td>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "      <td>SDYNA is a general framework designed to addre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             terms  \\\n",
       "0                                        ['cs.LG']   \n",
       "1                               ['cs.LG', 'cs.AI']   \n",
       "2                    ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3                               ['cs.LG', 'cs.CR']   \n",
       "4                                        ['cs.LG']   \n",
       "...                                            ...   \n",
       "56176                           ['cs.CV', 'cs.IR']   \n",
       "56177  ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']   \n",
       "56178                                    ['cs.LG']   \n",
       "56179              ['stat.ML', 'cs.LG', 'math.OC']   \n",
       "56180                ['cs.LG', 'cs.AI', 'stat.ML']   \n",
       "\n",
       "                                                  titles  \\\n",
       "0      Multi-Level Attention Pooling for Graph Neural...   \n",
       "1      Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2      Power up! Robust Graph Convolutional Network v...   \n",
       "3      Releasing Graph Neural Networks with Different...   \n",
       "4      Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "...                                                  ...   \n",
       "56176  Mining Spatio-temporal Data on Industrializati...   \n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...   \n",
       "56178  Deep Reinforcement Learning with Double Q-lear...   \n",
       "56179                        Generalized Low Rank Models   \n",
       "56180  Chi-square Tests Driven Method for Learning th...   \n",
       "\n",
       "                                               abstracts  \n",
       "0      Graph neural networks (GNNs) have been widely ...  \n",
       "1      Deep networks and decision forests (such as ra...  \n",
       "2      Graph convolutional networks (GCNs) are powerf...  \n",
       "3      With the increasing popularity of Graph Neural...  \n",
       "4      Machine learning solutions for pattern classif...  \n",
       "...                                                  ...  \n",
       "56176  Despite the growing availability of big data i...  \n",
       "56177  This paper presents a simple end-to-end model ...  \n",
       "56178  The popular Q-learning algorithm is known to o...  \n",
       "56179  Principal components analysis (PCA) is a well-...  \n",
       "56180  SDYNA is a general framework designed to addre...  \n",
       "\n",
       "[56181 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9793536f",
   "metadata": {},
   "source": [
    "# **Data Cleaning and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916611d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f26f2c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "terms        0\n",
       "titles       0\n",
       "abstracts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec439c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547e3c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          ['cs.LG']\n",
       "1                                 ['cs.LG', 'cs.AI']\n",
       "2                      ['cs.LG', 'cs.CR', 'stat.ML']\n",
       "3                                 ['cs.LG', 'cs.CR']\n",
       "4                                          ['cs.LG']\n",
       "                            ...                     \n",
       "56176                             ['cs.CV', 'cs.IR']\n",
       "56177    ['cs.LG', 'cs.AI', 'cs.CL', 'I.2.6; I.2.7']\n",
       "56178                                      ['cs.LG']\n",
       "56179                ['stat.ML', 'cs.LG', 'math.OC']\n",
       "56180                  ['cs.LG', 'cs.AI', 'stat.ML']\n",
       "Name: terms, Length: 56181, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data['terms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e153f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.LG' 'cs.AI' 'cs.CR' ... 'D.1.3; G.4; I.2.8; I.2.11; I.5.3; J.3'\n",
      " '68T07, 68T45, 68T10, 68T50, 68U35' 'I.2.0; G.3']\n",
      "lenght : 1177\n"
     ]
    }
   ],
   "source": [
    "# getting unique labels\n",
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()\n",
    "print(\"labels :\",labels)\n",
    "print(\"lenght :\",len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee54d7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41105 rows in the deduplicated dataset.\n",
      "2503\n",
      "3401\n"
     ]
    }
   ],
   "source": [
    "# remove duplicate entries based on the \"titles\" (terms) column\n",
    "# This filters the DataFrame, keeping only the rows where the titles are not duplicated.\n",
    "arxiv_data = arxiv_data[~arxiv_data['titles'].duplicated()]\n",
    "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n",
    "# There are some terms with occurrence as low as 1.\n",
    "print(sum(arxiv_data['terms'].value_counts()==1))\n",
    "# how many unique terms\n",
    "print(arxiv_data['terms'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8ce112a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38602, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filtering the rare terms. (it keeps only those rows where the \"terms\" value occurs more than once in the original DataFrame.)\n",
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x: len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8d90cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['cs.LG']), list(['cs.LG', 'cs.AI']),\n",
       "       list(['cs.LG', 'cs.CR', 'stat.ML'])], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It evaluates the given string containing a Python literal or container display (e.g., a list or dictionary) and returns the corresponding Python object.\n",
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cbfb4",
   "metadata": {},
   "source": [
    "# **train and test split.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade1640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in training set: 34741\n",
      "Number of rows in validation set: 1930\n",
      "Number of rows in test set: 1931\n"
     ]
    }
   ],
   "source": [
    "test_split = 0.1\n",
    "\n",
    "# Initial train and test split.\n",
    "# The stratify parameter ensures that the splitting is done in a way that preserves the same distribution of labels (terms) in both the training and test sets.\n",
    "train_df, test_df = train_test_split(arxiv_data_filtered,test_size=test_split,stratify=arxiv_data_filtered[\"terms\"].values,)\n",
    "\n",
    "# Splitting the test set further into validation\n",
    "# and new test sets.\n",
    "val_df = test_df.sample(frac=0.5)\n",
    "test_df.drop(val_df.index, inplace=True)\n",
    "\n",
    "print(f\"Number of rows in training set: {len(train_df)}\")\n",
    "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
    "print(f\"Number of rows in test set: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d7e94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "\n",
      "['[UNK]', 'cs.CV', 'cs.LG', 'stat.ML', 'cs.AI', 'eess.IV', 'cs.RO', 'cs.CL', 'cs.NE', 'cs.GR', 'cs.CR', 'math.OC', 'eess.SP', 'cs.SI', 'cs.MM', 'cs.SY', 'cs.IR', 'eess.SY', 'cs.MA', 'cs.HC', 'math.IT', 'cs.IT', 'cs.DC', 'stat.AP', 'cs.CY', 'stat.ME', 'stat.TH', 'math.ST', 'eess.AS', 'cs.SD', 'cs.DS', 'q-bio.QM', 'q-bio.NC', 'cs.CG', 'stat.CO', 'cs.GT', 'cs.NI', 'math.NA', 'cs.SE', 'I.2.6', 'cs.NA', 'physics.chem-ph', 'cs.DB', 'physics.comp-ph', 'cs.LO', 'cond-mat.dis-nn', 'q-bio.BM', 'math.PR', 'cs.PL', '68T45', 'cs.AR', 'physics.data-an', 'quant-ph', 'I.2.10', 'cs.CE', 'cond-mat.stat-mech', 'q-fin.ST', 'math.DS', 'I.4.6', 'cs.CC', '68T05', 'physics.ao-ph', 'physics.soc-ph', 'physics.med-ph', 'cs.PF', 'cs.DM', 'q-bio.GN', 'econ.EM', 'I.4.8', 'astro-ph.IM', 'physics.flu-dyn', 'math.AT', 'hep-ex', 'cs.FL', 'I.4', '68U10', 'q-fin.TR', 'physics.geo-ph', 'I.5.4', 'I.2', 'cond-mat.mtrl-sci', 'I.4.9', '68T10', 'physics.optics', 'I.4; I.5', '68T07', 'q-fin.CP', 'math.AP', 'I.2.6; I.2.8', 'I.2.10; I.4; I.5', '65D19', 'q-bio.PE', 'physics.app-ph', 'nlin.CD', 'math.CO', 'cs.MS', 'I.4.5', 'I.2.6; I.5.1', 'I.2.0; I.2.6', '68U01', '68T01', 'q-fin.GN', 'hep-ph', 'cs.SC', 'cs.ET', 'K.3.2', 'I.2.8', '68T30', 'q-fin.EC', 'q-bio.MN', 'econ.GN', 'I.4.9; I.5.4', 'I.4.0', 'I.2; I.5', 'I.2; I.4; I.5', 'I.2.6; I.2.7', 'I.2.10; I.4.8', '68T99', '68Q32', '68', '62H30', 'q-fin.RM', 'q-fin.PM', 'q-bio.TO', 'q-bio.OT', 'physics.plasm-ph', 'physics.class-ph', 'physics.bio-ph', 'nlin.AO', 'math.SP', 'math.MP', 'math.LO', 'math.FA', 'math-ph', 'cs.DL', 'cond-mat.soft', 'I.5.2', 'I.4.6; I.4.8', 'I.4.4', 'I.4.3', 'I.4.1', 'I.3.7', 'I.2; J.2', 'I.2; I.2.6; I.2.7', 'I.2.7', 'I.2.6; I.5.4', 'I.2.6; I.2.9', 'I.2.6; I.2.7; H.3.1; H.3.3', 'I.2.6; I.2.10', 'I.2.6, I.5.4', 'I.2.1; J.3', 'I.2.10; I.5.1; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.2.6', 'I.2.1', 'H.3.1; I.2.6; I.2.7', 'H.3.1; H.3.3; I.2.6; I.2.7', 'G.3', 'F.2.2; I.2.7', 'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', '68Txx', '62H99', '62H35', '60L10, 60L20', '14J60 (Primary) 14F05, 14J26 (Secondary)']\n"
     ]
    }
   ],
   "source": [
    "# creates a TensorFlow RaggedTensor (terms) from the values in the \"terms\" column of the train_df DataFrame. A RaggedTensor is a tensor with non-uniform shapes\n",
    "terms = tf.ragged.constant(train_df['terms'].values)\n",
    "# This line creates a StringLookup layer in TensorFlow. The purpose of this layer is to map strings to integer indices and vice versa. The output_mode=\"multi_hot\" indicates that the layer will output a multi-hot encoded representation of the input strings.\n",
    "lookup = tf.keras.layers.StringLookup(output_mode='multi_hot')\n",
    "# This step adapts the StringLookup layer to the unique values in the \"terms\" column, building the vocabulary.\n",
    "lookup.adapt(terms)\n",
    "# retrieve vocabulary\n",
    "vocab = lookup.get_vocabulary()\n",
    "\n",
    "print(\"Vocabulary:\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6912fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original label: ['cs.CV']\n",
      "Label-binarized representation: [[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df[\"terms\"].iloc[0]\n",
    "print(f\"Original label: {sample_label}\")\n",
    "\n",
    "label_binarized = lookup([sample_label])\n",
    "print(f\"Label-binarized representation: {label_binarized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1653b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_seqlen: Maximum sequence length. It indicates the maximum length allowed for sequences.\n",
    "max_seqlen = 150\n",
    "#batch_size: Batch size. It specifies the number of samples to use in each iteration.\n",
    "batch_size = 128\n",
    "#padding_token: A token used for padding sequences.\n",
    "padding_token = \"<pad>\"\n",
    "#auto = tf.data.AUTOTUNE: auto is assigned the value tf.data.AUTOTUNE,\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # creating sequences of labesls\n",
    "    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n",
    "    #This line uses the previously defined lookup layer to convert the ragged tensor of labels into a binarized representation. The resulting label_binarized is a NumPy array.\n",
    "    label_binarized = lookup(labels).numpy()\n",
    "    # creating sequences of text.\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe[\"abstracts\"].values, label_binarized))\n",
    "    # shuffling data basis on condition\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "    return dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0437583",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_dataset(train_df, is_train=True)\n",
    "validation_dataset = make_dataset(val_df, is_train=False)\n",
    "test_dataset = make_dataset(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a51280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: b\"Technology and collaboration enable dramatic increases in the size of\\npsychological and psychiatric data collections, but finding structure in these\\nlarge data sets with many collected variables is challenging. Decision tree\\nensembles like random forests (Strobl, Malley, and Tutz, 2009) are a useful\\ntool for finding structure, but are difficult to interpret with multiple\\noutcome variables which are often of interest in psychology. To find and\\ninterpret structure in data sets with multiple outcomes and many predictors\\n(possibly exceeding the sample size), we introduce a multivariate extension to\\na decision tree ensemble method called Gradient Boosted Regression Trees\\n(Friedman, 2001). Our method, multivariate tree boosting, can be used for\\nidentifying important predictors, detecting predictors with non-linear effects\\nand interactions without specification of such effects, and for identifying\\npredictors that cause two or more outcome variables to covary without\\nparametric assumptions. We provide the R package 'mvtboost' to estimate, tune,\\nand interpret the resulting model, which extends the implementation of\\nunivariate boosting in the R package 'gbm' (Ridgeway, 2013) to continuous,\\nmultivariate outcomes. To illustrate the approach, we analyze predictors of\\npsychological well-being (Ryff and Keyes, 1995). Simulations verify that our\\napproach identifies predictors with non-linear effects and achieves high\\nprediction accuracy, exceeding or matching the performance of (penalized)\\nmultivariate multiple regression and multivariate decision trees over a wide\\nrange of conditions.\"\n",
      "Label(s): ['cs.LG' 'stat.ML']\n",
      " \n",
      "Abstract: b'Several recent approaches showed how the representations learned by\\nConvolutional Neural Networks can be repurposed for novel tasks. Most commonly\\nit has been shown that the activation features of the last fully connected\\nlayers (fc7 or fc6) of the network, followed by a linear classifier outperform\\nthe state-of-the-art on several recognition challenge datasets. Instead of\\nrecognition, this paper focuses on the image retrieval problem and proposes a\\nexamines alternative pooling strategies derived for CNN features. The presented\\nscheme uses the features maps from an earlier layer 5 of the CNN architecture,\\nwhich has been shown to preserve coarse spatial information and is semantically\\nmeaningful. We examine several pooling strategies and demonstrate superior\\nperformance on the image retrieval task (INRIA Holidays) at the fraction of the\\ncomputational cost, while using a relatively small memory requirements. In\\naddition to retrieval, we see similar efficiency gains on the SUN397 scene\\ncategorization dataset, demonstrating wide applicability of this simple\\nstrategy. We also introduce and evaluate a novel GeoPlaces5K dataset from\\ndifferent geographical locations in the world for image retrieval that stresses\\nmore dramatic changes in appearance and viewpoint.'\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b'A novel kernel-based support vector machine (SVM) for graph classification is\\nproposed. The SVM feature space mapping consists of a sequence of graph\\nconvolutional layers, which generates a vector space representation for each\\nvertex, followed by a pooling layer which generates a reproducing kernel\\nHilbert space (RKHS) representation for the graph. The use of a RKHS offers the\\nability to implicitly operate in this space using a kernel function without the\\ncomputational complexity of explicitly mapping into it. The proposed model is\\ntrained in a supervised end-to-end manner whereby the convolutional layers, the\\nkernel function and SVM parameters are jointly optimized with respect to a\\nregularized classification loss. This approach is distinct from existing\\nkernel-based graph classification models which instead either use feature\\nengineering or unsupervised learning to define the kernel function.\\nExperimental results demonstrate that the proposed model outperforms existing\\ndeep learning baseline models on a number of datasets.'\n",
      "Label(s): ['cs.LG' 'stat.ML']\n",
      " \n",
      "Abstract: b\"Self-supervised representation learning approaches have recently surpassed\\ntheir supervised learning counterparts on downstream tasks like object\\ndetection and image classification. Somewhat mysteriously the recent gains in\\nperformance come from training instance classification models, treating each\\nimage and it's augmented versions as samples of a single class. In this work,\\nwe first present quantitative experiments to demystify these gains. We\\ndemonstrate that approaches like MOCO and PIRL learn occlusion-invariant\\nrepresentations. However, they fail to capture viewpoint and category instance\\ninvariance which are crucial components for object recognition. Second, we\\ndemonstrate that these approaches obtain further gains from access to a clean\\nobject-centric training dataset like Imagenet. Finally, we propose an approach\\nto leverage unstructured videos to learn representations that possess higher\\nviewpoint invariance. Our results show that the learned representations\\noutperform MOCOv2 trained on the same data in terms of invariances encoded and\\nthe performance on downstream image classification and semantic segmentation\\ntasks.\"\n",
      "Label(s): ['cs.CV']\n",
      " \n",
      "Abstract: b'A new modification of the Neural Additive Model (NAM) called SurvNAM and its\\nmodifications are proposed to explain predictions of the black-box machine\\nlearning survival model. The method is based on applying the original NAM to\\nsolving the explanation problem in the framework of survival analysis. The\\nbasic idea behind SurvNAM is to train the network by means of a specific\\nexpected loss function which takes into account peculiarities of the survival\\nmodel predictions and is based on approximating the black-box model by the\\nextension of the Cox proportional hazards model which uses the well-known\\nGeneralized Additive Model (GAM) in place of the simple linear relationship of\\ncovariates. The proposed method SurvNAM allows performing the local and global\\nexplanation. A set of examples around the explained example is randomly\\ngenerated for the local explanation. The global explanation uses the whole\\ntraining dataset. The proposed modifications of SurvNAM are based on using the\\nLasso-based regularization for functions from GAM and for a special\\nrepresentation of the GAM functions using their weighted linear and non-linear\\nparts, which is implemented as a shortcut connection. A lot of numerical\\nexperiments illustrate the SurvNAM efficiency.'\n",
      "Label(s): ['cs.LG' 'stat.ML']\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def invert_multi_hot(encoded_labels):\n",
    "    hot_indeces=np.argwhere(encoded_labels==1.0)[...,0]\n",
    "    return np.take(vocab,hot_indeces)\n",
    "\n",
    "text_batch, label_batch = next(iter(train_dataset))\n",
    "for i, text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None, ...]\n",
    "    print(f\"Abstract: {text}\")\n",
    "    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cec32f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159119\n"
     ]
    }
   ],
   "source": [
    "# Creating vocabulary with uniques words\n",
    "vocabulary = set()\n",
    "train_df[\"abstracts\"].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ff29b",
   "metadata": {},
   "source": [
    "# ** =======Section 2========**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b38235",
   "metadata": {},
   "source": [
    "# **Recommendation System**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9d9830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data.drop(columns = [\"terms\",\"abstracts\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ad83770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56176</th>\n",
       "      <td>Mining Spatio-temporal Data on Industrializati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56177</th>\n",
       "      <td>Wav2Letter: an End-to-End ConvNet-based Speech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56178</th>\n",
       "      <td>Deep Reinforcement Learning with Double Q-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56179</th>\n",
       "      <td>Generalized Low Rank Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56180</th>\n",
       "      <td>Chi-square Tests Driven Method for Learning th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56181 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles\n",
       "0      Multi-Level Attention Pooling for Graph Neural...\n",
       "1      Decision Forests vs. Deep Networks: Conceptual...\n",
       "2      Power up! Robust Graph Convolutional Network v...\n",
       "3      Releasing Graph Neural Networks with Different...\n",
       "4      Recurrence-Aware Long-Term Cognitive Network f...\n",
       "...                                                  ...\n",
       "56176  Mining Spatio-temporal Data on Industrializati...\n",
       "56177  Wav2Letter: an End-to-End ConvNet-based Speech...\n",
       "56178  Deep Reinforcement Learning with Double Q-lear...\n",
       "56179                        Generalized Low Rank Models\n",
       "56180  Chi-square Tests Driven Method for Learning th...\n",
       "\n",
       "[56181 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8e3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "056f3632",
   "metadata": {},
   "source": [
    "# **Sentence Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13b65715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najnin/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer,util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e3c301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/najnin/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aa10c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = arxiv_data['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae7cde06",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83d5ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06643411, -0.04954598,  0.06388082, ...,  0.00106297,\n",
       "        -0.12156384, -0.06962778],\n",
       "       [ 0.0921226 , -0.07606938,  0.06572865, ..., -0.0856517 ,\n",
       "        -0.09266545,  0.00725295],\n",
       "       [-0.08162681,  0.02428927,  0.01888743, ...,  0.00806166,\n",
       "        -0.05129531, -0.05873994],\n",
       "       ...,\n",
       "       [-0.09695324,  0.00057095,  0.07726481, ..., -0.01443811,\n",
       "        -0.04748208,  0.06130565],\n",
       "       [ 0.00768868, -0.10124182,  0.08909856, ..., -0.08199871,\n",
       "        -0.05649743,  0.09007054],\n",
       "       [ 0.06078521, -0.08312801, -0.00907773, ..., -0.03148183,\n",
       "         0.0571311 ,  0.0569689 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36eafd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ac5188",
   "metadata": {},
   "source": [
    "# **Print the embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb976d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Lifelong Graph Learning\n",
      "Embedding length: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "\n",
    "for sentence,embedding in zip(sentences,embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding length:\", len(embedding)) # list of floats\n",
    "    print(\"\")\n",
    "    if c>=5:\n",
    "        break\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78fc789",
   "metadata": {},
   "source": [
    "# **Save files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b79c045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Models/embeddings.pkl\",'wb') as f:\n",
    "    pickle.dump(embeddings,f)\n",
    "    \n",
    "with open('Models/sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "    \n",
    "with open('Models/rec_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ad793",
   "metadata": {},
   "source": [
    "# **Recommendation for similar papers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1adb2efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load save files\n",
    "embeddings = pickle.load(open('Models/embeddings.pkl','rb'))\n",
    "sentences = pickle.load(open('Models/sentences.pkl','rb'))\n",
    "rec_model = pickle.load(open('Models/rec_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dfaeb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    # Calculate cosine similarity scores between the embeddings of input_paper and all papers in the dataset.\n",
    "    cosine_scores = util.cos_sim(embeddings, rec_model.encode(input_paper))\n",
    "    \n",
    "    # Get the indices of the top-k most similar papers based on cosine similarity.\n",
    "    top_similar_papers = torch.topk(cosine_scores, dim=0, k=5, sorted=True)\n",
    "                                 \n",
    "    # Retrieve the titles of the top similar papers.\n",
    "    papers_list = []\n",
    "    for i in top_similar_papers.indices:\n",
    "        papers_list.append(sentences[i.item()])\n",
    "    \n",
    "    return papers_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09ed8304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper............\n",
      "=============================================\n",
      "Attention that does not Explain Away\n",
      "Attention that does not Explain Away\n",
      "Attention that does not Explain Away\n",
      "Area Attention\n",
      "Area Attention\n"
     ]
    }
   ],
   "source": [
    "# exampel usage 1: (use this paper as input (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding))\n",
    "# exampel usage 2: (use this paper as input (Review of deep learning: concepts, CNN architectures, challenges, applications, future directions))\n",
    "\n",
    "input_paper = input(\"Enter the title of any paper you like \")\n",
    "\n",
    "recommend_papers = recommendation(input_paper)\n",
    "\n",
    "\n",
    "print(\"We recommend to read this paper............\")\n",
    "print(\"=============================================\")\n",
    "for paper in recommend_papers:\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8ff11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
